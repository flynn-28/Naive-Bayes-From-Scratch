{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "345f5cfc-3067-44fc-93b6-c25ad1058753",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier From Scratch\n",
    "\n",
    "This notebook demonstrates how to create, fit, and evaluate a *Naive Bayes Classifier* from scratch in Python. It explains the background and implementation to provide an understanding of the use cases of a Naive Bayes Classifier and how they work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c06f498-3d7e-40ee-aebb-4c26b7fec9dc",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "A **Naive Bayes Classifier** is a very simple machine learning algorithm used for *classification*, or predicting the class of a given data point. It is based on the *Bayes Theorem*, an equation that updates the probability of a result as more information becomes available. \n",
    "\n",
    "### Bayesâ€™ Theorem\n",
    "\n",
    "The Bayes Theorem defines the following formula:\n",
    "\n",
    "$$\n",
    "P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $A$ and $B$ are events\n",
    "- $P(B) \\neq 0$ to ensure the denominator is valid\n",
    "- $P(A|B)$ is the *posterior probability* or the probability of $A$ happening if $B$ is true\n",
    "- $P(B|A)$ is the *likelihood* or the probability of seeing $B$ if $A$ is true\n",
    "- $P(A)$ is the *prior probability* of $A$ or the belief before seeing evidence\n",
    "- $P(B)$ is the *marginal probability* of $B$ or the total probability of observing $B$\n",
    "\n",
    "### The Naive Assumption\n",
    "\n",
    "Naive Bayes assumes that every feature is unrelated to the others as long as the class label is known. This is represented mathematically as:\n",
    "\n",
    "$$\n",
    "P(x_1, x_2, ..., x_n | y) = P(x_1 | y) \\cdot P(x_2 | y) \\cdot ... \\cdot P(x_n | y)\n",
    "$$\n",
    "\n",
    "While this assumption of independence is rarely true in the real world, it has several use cases, such as\n",
    "- sentiment analysis\n",
    "- text classification\n",
    "- image recognition\n",
    "\n",
    "Or real world examples including:\n",
    "- Fraud Detection\n",
    "- Medical Diagnosis\n",
    "- Content Recommendation\n",
    "\n",
    "### Types of Naive Bayes Classifiers\n",
    "\n",
    "Depending on the type of input data, a different variant of Naive Bayes is used. The three most common types are:\n",
    "\n",
    "* **Gaussian**: assuming the features are distributed normally\n",
    "* **Multinomial**: assuming features are counts or frequencies\n",
    "* **Bernoulli**: assuming features are binary or boolean\n",
    "\n",
    "This notebook demonstrates how to create a Gaussian Naive Bayes Classifier and fit it on synthetic data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8010f576-9641-4215-a42d-dc38a44615a0",
   "metadata": {},
   "source": [
    "## Step 1: Data Generation\n",
    "\n",
    "The code below creates the `generate_data` function with the following parameters:\n",
    "\n",
    "- `n_samples`: the number of samples or data points to generate\n",
    "- `n_features`: the number of features or columns each sample has\n",
    "- `n_classes`: the number of classes or categories\n",
    "\n",
    "This function generates a synthetic dataset we can use to train our model on. You can change the length and complexity of the dataset by calling it with different values. The function does the following to generate data for each class:\n",
    "\n",
    "- the mean of each feature is randomly selected\n",
    "- variability is added using a diagonal covariance matrix\n",
    "- random data points around the mean are generated\n",
    "- noise is added to the samples\n",
    "- samples are labeled based on class\n",
    "\n",
    "After data is generated for each class, the function returns the features as `X` and the labels as `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2171722e-97f2-4c2c-8e2c-b2c123fdf562",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # import NumPy for math and arrays\n",
    "\n",
    "def generate_data(n_samples=1000, n_features=3, n_classes=3):\n",
    "    X = [] # store features\n",
    "    y = [] # store labels\n",
    "\n",
    "    # generate for each class\n",
    "    for i in range(n_classes):\n",
    "        mean = np.random.uniform(-5, 5, size=n_features)              # random mean for class\n",
    "        covariance = np.eye(n_features) * np.random.uniform(0.5, 1.5) # randomly scalled covariance matrix\n",
    "        # generate samples\n",
    "        samples = np.random.multivariate_normal(mean, covariance, size=n_samples // n_classes)\n",
    "        noise = np.random.normal(0, 0.1, samples.shape) # calculate noise to add\n",
    "        samples += noise                                # add noise\n",
    "        X.append(samples)                               # append to features\n",
    "        y.extend([i] * (n_samples // n_classes))        # add labels\n",
    "    return np.vstack(X), np.array(y)                    # return data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1981b6f-73c1-4607-b4c5-00969d141585",
   "metadata": {},
   "source": [
    "We also want to be able to train and test our model using different data. To do this, we will define the function `split_data`, to split our features and labels into train and test sets. The function takes `X` and `y` as inputs as well as `test_size`, used to determine the amount of samples in the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac2abb2f-2d2e-4c66-bcc8-e8ba5045a56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X, y, test_size=0.2):\n",
    "    # sample indices\n",
    "    indices = [i for i in range(len(X))]        # list indices corresponding to samples in X\n",
    "    np.random.shuffle(indices)                  # shuffle indices for random split\n",
    "    # where to split data\n",
    "    split_index = int(len(X) * (1 - test_size)) # where to split the data \n",
    "    train_index = indices[:split_index]         # select indices for training set\n",
    "    test_index = indices[split_index:]          # select indices for testing seu\n",
    "    # train samples\n",
    "    X_train = X[train_index]                    # get training features\n",
    "    y_train = y[train_index]                    # get training labels\n",
    "    # test samples\n",
    "    X_test = X[test_index]                      # get testing features\n",
    "    y_test = y[test_index]                      # get testing labels\n",
    "    return X_train, X_test, y_train, y_test     # return train/test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de31f5d7-29ef-444e-acdc-a66f48eea5ee",
   "metadata": {},
   "source": [
    "### Step 2: Model Fitting\n",
    "\n",
    "Naive Bayes classifiers are trained differently than regression models. Instead of optimizing parameters over many iterations, they learn instantly by summarizing the training data by calculating various statistics for each class.\n",
    "\n",
    "The `fit` function is defined below to train our model. It starts by identifying each unique class and initializing a dict to store class statistics. It then iterates through the classes doing the following for each one:\n",
    "\n",
    "- seperate samples labeled as the current class\n",
    "- calculate the mean and variance for each feature\n",
    "- calculate the prior probability, or the likelihood of the label before looking at the data\n",
    "- save the classes statistics to the dictionary\n",
    "\n",
    "Finally, the function returns the dictionary containing each classes summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c2268c2-5b63-4ac7-825c-ab12b30c74c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(X, y):                                     # function to fit model on data\n",
    "    classes = np.unique(y)                         # get each unique class\n",
    "    stats = {}                                     # dict to store stats for each class\n",
    "    for i in classes:                              # iterate through each class\n",
    "        class_X = X[y == i]                        # get all samples with the class\n",
    "        stats[i] = {                               # add stats for class\n",
    "            \"mean\": class_X.mean(axis=0),          # average of each feature in class\n",
    "            \"var\": class_X.var(axis=0),            # variance of each feature in class\n",
    "            \"prior\": class_X.shape[0] / X.shape[0] # estimated probability of class\n",
    "        }\n",
    "    return stats                                   # return data stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b5dcdf-6a14-4fdd-be1b-7513ae82f5f6",
   "metadata": {},
   "source": [
    "## Step 3: Probability Density Function\n",
    "\n",
    "Probability density functions are used to calculate the likelihood of a feature value given a class. Since our data is Gaussian, or distributed normally, we use the Gaussian Probability Density Function:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "f(x) = \\frac{1}{\\sigma \\sqrt{2\\pi} } e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $x$ is the feature we are evaluating\n",
    "- $\\mu$ is the average of that feature for a given class\n",
    "- $\\sigma$ is the standard deviation of that feature for the class\n",
    "\n",
    "In our implementation, we used variance instead of standard deviation and included an epsilon, `eps`, or a very small value just larger than zero, added to avoid division by zero. The function begins by calculating the numerator, then the denominator. Finally, it returns the quotient of the numerator and denominator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a0821b2-2637-4eaa-b755-7d3ae32658a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf(x, mean, var):                                    # compute probability density with class mean and variance\n",
    "    eps = 1e-6                                            # epsilon to avoid division by 0\n",
    "    numerator = np.exp(- (x - mean)**2 / (2 * var + eps)) # calculate numerator of the pdf\n",
    "    denominator = np.sqrt(2 * np.pi * var + eps)          # calculate pdf denominator\n",
    "    return numerator / denominator                        # return probability density"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b72779a-de90-4342-9756-3d69e01ff2d4",
   "metadata": {},
   "source": [
    "## Step 4: Predict\n",
    "\n",
    "A Naive Bayes model classifies, or predicts, new samples by calculating the **Posterior Probability** for each class. This is done by combining:\n",
    "\n",
    "- The *Prior Probability* of each class, or how likely it is overall, with\n",
    "- The *Likelihood* of the features for that class using the probability density function\n",
    "\n",
    "Below, we wrote the `predict` function with inputs `X` for the samples we want to classify and `stats` for the datasets summary we determined using the `fit` function.\n",
    "\n",
    "For each sample in `X`, it does the following:\n",
    "\n",
    "- calculates the log of the prior for each class\n",
    "- calculates the log likelihood of the features for each class\n",
    "- combines the log likelihoods of each feature\n",
    "- calculates the log-posterior probability by taking the sum of the prior and likelihood\n",
    "- selects the class with the highest log-posterior probability for the prediction\n",
    "\n",
    "After this is done, the function returns an array of the predicted classes of `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3822047a-bec7-4c03-8eec-9dde1fc23de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, stats):                                                  # predict classes based on data stats\n",
    "    predictions = []                                                    # list to store predictions\n",
    "    for i in X:                                                         # iterate through samples\n",
    "        posteriors = []                                                 # store posteriors or updated probabilities\n",
    "        for j, params in stats.items():                                 # iterate through each class and parameter\n",
    "            prior = np.log(params[\"prior\"])                             # log of classes prior\n",
    "            probability_density = pdf(i, params[\"mean\"], params[\"var\"]) # calculate probability density\n",
    "            likelihood = np.sum(np.log(probability_density))            # calculate class likelihood\n",
    "            posteriors.append(prior + likelihood)                       # calculate and store posterior probability\n",
    "        predictions.append(np.argmax(posteriors))                       # predict class with highest likelihood and append\n",
    "    return np.array(predictions)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1d9f8a-f674-480f-b10f-7e76023942fd",
   "metadata": {},
   "source": [
    "## Step 5: Using the Code\n",
    "\n",
    "Now we can use our code to create and fit our model.\n",
    "\n",
    "**First, we generate and split our data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c62ceb4-c000-414e-b467-a6fb65672064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data\n",
    "X, y = generate_data()\n",
    "X_train, X_test, y_train, y_test = split_data(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbe30e3-58e1-449a-b05d-59cc080f1c67",
   "metadata": {},
   "source": [
    "**Next, we can fit the model on the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb32152c-1e54-4561-903a-8b2169396d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit\n",
    "model_stats = fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f574fe-c81b-4be5-8d55-087f5fccf66e",
   "metadata": {},
   "source": [
    "**Then, we predict our testing data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c2f3711-337f-43cb-889d-876e19187753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "predictions = predict(X_test, model_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f090a5e-cd55-431f-94ef-972929df3d83",
   "metadata": {},
   "source": [
    "**Finally, we can evaluate the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "814f8b69-983c-4c38-9a13-0787262345eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.96\n"
     ]
    }
   ],
   "source": [
    "# Accuracy\n",
    "accuracy = np.mean(predictions == y_test)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c8944d-9d72-4cb7-876b-531495c2468c",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Background**\n",
    "- Naive Bayes is a classifier based on the *Bayes Theorem*\n",
    "- It assumes features are independent of one another given the class\n",
    "- Used for text classification, sentiment analysis, and images detection\n",
    "\n",
    "**Types of Naive Bayes**\n",
    "- *Gaussian* or normally distributed features\n",
    "- *Multinomial* or frequency based features\n",
    "- *Bernoulli* or binary features\n",
    "\n",
    "**Implementation**\n",
    "- *Data*: synthetically generate a Gaussian dataset\n",
    "- *Fitting*: calculate mean, variance, and prior probability for each class\n",
    "- *Prediction*: calculate posterior probability using the gaussian probability density function\n",
    "- *Evaluation*: measure models performance using accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3fa7af-8b3f-4689-b54d-87c938a07d19",
   "metadata": {},
   "source": [
    "## Author and Liscense\n",
    "\n",
    "This notebook was authored by Aiden Flynn and is available under the [Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0.txt) Liscense.\n",
    "\n",
    "[Kaggle](https://www.kaggle.com/flynn28) | [Github](https://github.com/flynn-28/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
